# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wExGcGQIZUUxh94NXZ2i154adDr6ZU41
"""

from google.colab import drive
drive.mount('/content/gdrive')

import numpy as np

# npy 파일에서 배열 불러오기
x_train = np.load('/content/gdrive/MyDrive/Yonsei-vnl-coding-assignment-vision-48hrs/dataset/train_xdata.npy')  # 수화 입력 이미지 데이터를 저장한 npy 파일
y_train = np.load('/content/gdrive/MyDrive/Yonsei-vnl-coding-assignment-vision-48hrs/dataset/train_ydata.npy')  # 이미지 데이터에 해당하는 레이블(클래스) 정보를 저장한 npy 파일

print(x_train.shape)  # x_data = (2062, 64, 64)
print(y_train.shape)  # y_data = (2062, 10)

# npy 파일에서 배열 불러오기
x_val = np.load('/content/gdrive/MyDrive/Yonsei-vnl-coding-assignment-vision-48hrs/dataset/val_xdata.npy')  # 수화 입력 이미지 데이터를 저장한 npy 파일
y_val = np.load('/content/gdrive/MyDrive/Yonsei-vnl-coding-assignment-vision-48hrs/dataset/val_ydata.npy')  # 이미지 데이터에 해당하는 레이블(클래스) 정보를 저장한 npy 파일

print(x_val.shape)  # x_data = (2062, 64, 64)
print(y_val.shape)  # y_data = (2062, 10)

x_test = np.load('/content/gdrive/MyDrive/Yonsei-vnl-coding-assignment-vision-48hrs/dataset/test_xdata.npy')  # 수화 입력 이미지 데이터를 저장한 npy 파일
y_test = np.load('/content/gdrive/MyDrive/Yonsei-vnl-coding-assignment-vision-48hrs/dataset/test_ydata.npy')  # 이미지 데이터에 해당하는 레이블(클래스) 정보를 저장한 npy 파일

print(x_test.shape)  # x_data = (2062, 64, 64)
print(y_test.shape)  # y_data = (2062, 10)


# shape 변경
x_train = np.transpose(x_train, (0, 3, 1, 2))
print('변경된 shape: ', x_train.shape)

x_val = np.transpose(x_val, (0, 3, 1, 2))
print('변경된 shape: ', x_val.shape)

x_test = np.transpose(x_test, (0, 3, 1, 2))
print('변경된 shape: ', x_test.shape)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader, Dataset, SubsetRandomSampler

x_data_combined = np.concatenate((x_test, x_val), axis=0)
y_data_combined = np.concatenate((y_test, y_val), axis=0)

print(x_data_combined.shape)
print(y_data_combined.shape)


# validation set x 데이터와 y 데이터 합치기
# 데이터셋 객체 생성
class CustomDataset(Dataset):
    def __init__(self, data, targets):
        self.data = data
        self.targets = targets

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.targets[idx]

# 데이터셋 객체 생성
test_data = CustomDataset(x_data_combined, y_data_combined)
dataset = CustomDataset(x_train, y_train)

test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=32, shuffle=False)

# <네번째 모델구조>
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=2)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, stride=1, padding=2)
        self.conv3 = nn.Conv2d(in_channels=256, out_channels=1024, kernel_size=3, stride=1, padding=2)
        self.conv4 = nn.Conv2d(in_channels=1024, out_channels=4096, kernel_size=3, stride=1, padding=2)
        self.fc1 = nn.Linear(36864, 800)  # 입력 크기 수정
        self.fc2 = nn.Linear(800, 500)
        self.fc3 = nn.Linear(500, 100)
        self.relu = nn.ReLU()
        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout(0.5)  # Add dropout layer with dropout probability of 0.5

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.max_pool2d(x)
        x = self.relu(self.conv2(x))
        x = self.max_pool2d(x)
        x = self.relu(self.conv3(x))
        x = self.max_pool2d(x)
        x = self.relu(self.conv4(x))
        x = self.max_pool2d(x)

        x = x.view(x.size(0), -1)   # [batch_size, 50, 4, 4] -> [batch_size, 50*4*4]
        x = self.relu(self.fc1(x))
        x = self.dropout(x)  # Apply dropout layer to the fully connected layer
        x = self.fc2(x)
        x = self.fc3(x)

        return x

# Label Smoothing을 위한 손실 함수 정의
class LabelSmoothingLoss(nn.Module):
    def __init__(self, smoothing=0.1, num_classes=100):
        super(LabelSmoothingLoss, self).__init__()
        self.smoothing = smoothing
        self.num_classes = num_classes

    def forward(self, x, target):
        confidence = 1.0 - self.smoothing
        log_probs = torch.nn.functional.log_softmax(x, dim=-1)

        # 정답 레이블에 대해서는 실제 레이블을 사용하고, 다른 레이블에 대해서는 균등한 확률을 가정하여 계산합니다.
        target_probs = torch.full_like(log_probs, self.smoothing / (self.num_classes - 1))
        target_probs.scatter_(1, target.unsqueeze(1), confidence)

        return torch.nn.functional.kl_div(log_probs, target_probs, reduction='batchmean')

# 가중치 초기화
# def initialize_weights(m):
#     if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
#         nn.init.xavier_uniform_(m.weight)  # Xavier 초기화 사용

def he_initialization(layer):
    if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):
        nn.init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='relu')
        if layer.bias is not None:
            nn.init.constant_(layer.bias, 0.0)

# 모델 생성
model = CNN()
model = model.cuda()

# 모델의 모든 레이어에 가중치 초기화 적용
model.apply(he_initialization)

# 손실 함수와 옵티마이저 정의
# criterion = nn.CrossEntropyLoss()
criterion = LabelSmoothingLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, confusion_matrix
from sklearn.model_selection import KFold

# Define k-fold cross-validation
fold_count = 5
kfold = KFold(n_splits=fold_count, shuffle=False)

EPOCHS = 8
train_accu = [[] for _ in range(EPOCHS)]
train_losses = [[] for _ in range(EPOCHS)]
eval_losses = [[] for _ in range(EPOCHS)]
eval_accu = [[] for _ in range(EPOCHS)]
best = (0, -1, 1e9, []) # (fold, valid_idx, loss, model.state)



# append result by fold -> by epoch
def add_fold_result(epoch, d, loss):
    if epoch in d:
        d[epoch].append(loss)
    else:
        d[epoch] = [loss]
    return d

def avg_group_by_fold(losses):
   avg_value = []
   for f in range(len(losses)):
       if not losses[f]: break
       avg_value.append(np.array(losses[f]).mean())
   #print(losses)
   return avg_value
   #return [np.array(losses[f_key]).mean() for f_key in losses]


# for tracking by fold
train_losses_by_fold = [[] for _ in range(fold_count)]
valid_losses_by_fold = [[] for _ in range(fold_count)]

for epoch in range(EPOCHS):
    running_loss = 0
    correct = 0
    total = 0

    for fold, (train_index, valid_index) in enumerate(kfold.split(dataset)):
        # Split dataset and loader
        train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)  # index 생성
        val_subsampler = torch.utils.data.SubsetRandomSampler(valid_index)  # index 생성
        train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, sampler=train_subsampler)
        valid_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, sampler=val_subsampler)

        N1N2 = 0
        N3 = 0
        wake = 0
        REM = 0
        if epoch == 0:
            print("train len : %d" % len(train_index))
            print("valid len :%d" % len(valid_index))

        running_loss = 0.0
        correct = 0
        total = 0

        # Train
        model.train()
        for index, (data, target) in enumerate(train_loader):
            data, target = data.cuda(), target.cuda()
            optimizer.zero_grad()  # 기울기 초기화
            output = model(data)
            loss = criterion(output, target)
            loss.backward()  # 역전파
            optimizer.step()

            running_loss += loss.item()

            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            total += target.size(0)
            correct += pred.eq(target.view_as(pred)).sum().item()



        train_loss = running_loss / len(train_index)
        train_losses_by_fold[fold].append(train_loss)
        accu = 100. * correct / total
        train_accu[epoch].append(accu)
        train_losses[epoch].append(train_loss)

        running_loss = 0
        correct = 0
        total = 0

        # valid loss (just for check)
        with torch.no_grad():
            for index, (data, target) in enumerate(valid_loader):
                data, target = data.cuda(), target.cuda()
                model.eval()

                output = model(data)

                loss = criterion(output, target)
                running_loss += loss.item()

                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
                total += target.size(0)
                correct += pred.eq(target.view_as(pred)).sum().item()

            val_loss = running_loss / len(valid_index)
            valid_losses_by_fold[fold].append(val_loss)
            accu = 100. * correct / total
            eval_losses[epoch].append(val_loss)
            eval_accu[epoch].append(accu)

        print('[%d] fold=%d, train loss: %.6f, valid loss: %.6f' % (epoch + 1, fold, train_loss, val_loss))

for epoch in range(1):
    model.eval()  # test case 학습 방지
    running_loss = 0
    correct = 0
    total = 0

    y_true = []
    y_pred = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.cuda(), target.cuda()
            output = model(data)

            loss = criterion(output, target)
            running_loss += loss.item()

            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            total += target.size(0)
            correct += pred.eq(target.view_as(pred)).sum().item()

            output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()
            y_pred.extend(output)

            target = target.data.cpu().numpy()
            y_true.extend(target)

    test_loss = running_loss / len(test_loader.dataset)
    accu = 100. * correct / total
    print('Test Loss: %.3f | Accuracy: %.3f' % (test_loss, accu))

    print(np.shape(y_true))
    print(np.shape(y_pred))

    print(y_true[1])
    print(y_pred[1])

    # Generate a list of labels from 0 to 99
    labels = list(range(100))

    cm = confusion_matrix(y_true, y_pred, labels=labels)
    print(cm)

    # Calculate the accuracy for each class
    class_accuracy = np.diag(cm) / cm.sum(axis=1)

    # Print the 2D array of class accuracies
    print(class_accuracy)


plt.figure(figsize=(10, 5))
plt.plot(avg_group_by_fold(train_losses), label="train loss")
plt.plot(avg_group_by_fold(eval_losses), label="test loss")
plt.xlabel('epoch')
plt.ylabel('losses')
plt.legend(['Train', 'Valid'])
plt.title('Train vs Valid Losses')
plt.show()

plt.figure(figsize=(25, 5))
for fold in range(fold_count):
    plt.subplot(1, fold_count, fold + 1)
    plt.plot(train_losses_by_fold[fold], label="train loss")
    plt.plot(valid_losses_by_fold[fold], label="valid loss")
    plt.title(f'Loss (fold={fold})')
    plt.xlabel("epoch")
    plt.axvline(best[0], color='red')
    plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(avg_group_by_fold(train_accu), label="train accuracy")
plt.plot(avg_group_by_fold(eval_accu), label="test accuracy")
plt.xlabel("epoch")
plt.ylabel("accuracy")
plt.legend(['Train', 'Valid'])
plt.title("Train vs Valid Accuracy")
plt.show()